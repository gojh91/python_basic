{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gym.openai.com/\n",
    "# https://gym.openai.com/docs/\n",
    "try:\n",
    "    import gym\n",
    "    \n",
    "except ModuleNotFoundError as e:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install gym\n",
    "    import gym\n",
    "finally:\n",
    "    from gym.envs.registration import register\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Deteministic 환경."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 게임 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gym 환경설정\n",
    "# https://gym.openai.com/envs/ 에 들어가면 다양한 환경을 살펴볼 수 있음\n",
    "# Frozen-Lake : https://gym.openai.com/envs/FrozenLake-v0/ \n",
    "register(\n",
    "    id = 'FrozenLake-v1', \n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv', \n",
    "    kwargs={\n",
    "        # 'is_slippery': False 이면 Deteministic 환경.\n",
    "        # 'is_slippery': True 이면 Stochastic 환경.\n",
    "        'map_name' : '4x4', # 4x4 크기의 맵\n",
    "        'is_slippery' : False # 미끄러질 가능성 없음 (나중에 다룰 것)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 사용법 익히기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(16), Discrete(4), 16, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space, env.observation_space.n, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LEFT = 0\n",
    "# DOWN = 1\n",
    "# RIGHT = 2\n",
    "# UP = 3\n",
    "\n",
    "# 임의로 방향 하나를 선택함.\n",
    "action = env.action_space.sample()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "observation, reward, done, info = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "observation, reward, done, info = env.step(0)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위치:  0\n",
      "보상:  0.0\n",
      "Game Over?:  False\n",
      "정보(확률):  {'prob': 1.0}\n",
      "행동:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"위치: \", observation)\n",
    "print(\"보상: \",reward)\n",
    "print(\"Game Over?: \",done)\n",
    "print(\"정보(확률): \", info) # 내가 의도한대로 갈 확률\n",
    "print(\"행동: \", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기화\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 러닝이란"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q-learning에서는 앞으로 어떤 action을 취할지 결정하기 위해서 Q-table을 사용하는데,\n",
    "Q-table은 맵과 같은 모양(Frozen-Lake의 경우 4x4)으로 만들어지며\n",
    "각 칸에는 해당 칸에서 각각의 action을 취할 때 기대하는 reward 값으로 채워진다.\n",
    "(한 state에서 취할 수 있는 action이 상하좌우 4개이기 때문에 각 칸마다 4개의 값이 필요하다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning을 진행하면서 Q-table의 각 reward 값을 업데이트 하여 최상의 결과값을 리턴할 수 있도록 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy Q-learning은 0으로 채워놓은 Q-테이블에서 시작해서 각 state에서 취할 action을 랜덤으로 선택한다.\n",
    "(상하좌우 모두 reward가 0이기 때문에)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen-Lake에서는 결승점에 도달할 때만 reward 1이 주어지기 때문에 결승점에 도착하기 전에는\n",
    "reward는 무조건 0이 리턴된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action은 랜덤으로 정해지기 때문에 보통 구멍에 빠져서 게임이 종료되는 상황이 \n",
    "반복 되다가 우연히 결승점에 다다르면 그제서야 reward가 1이 주어진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 학습하는 agent는 이 전 칸에서 어떤 action(예를 들어 14번칸에서 오른쪽)을 하면 결승점에 다다른다는 것을\n",
    "알게 된다. 그럼 이 전 칸의 Q-table을 업데이트 할 수 있다(14번칸에서 오른쪽의 값을 1로 업데이트)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 Q-table은 우연히 결승점에 도착한 상황을 토대로 시작점까지 거꾸로 거슬러 올라가면서 Q값을 업데이트하는\n",
    "방식을 사용한다. (결승점에 도착하기 위해서는 14번에서 오른쪽으로 가야한다. 14번에 다다르기 위해서는 \n",
    "10번에서 아래로 가야한다. 10번에 가기 위해서는 ... ) 이런 알고리즘을 식으로 표현하면 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q[state, action] = reward + max(Q[new_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<< Dummy Q-learning >>  \n",
    "2강의 Dummy Q-learning에서는 1이 채워지지 않는 칸에서 action이 랜덤으로 정해지기 때문에 \n",
    "갈림길의 경우에는 랜덤으로 돌아다니다가 어떤 길로 우연히 처음 들게 되었는지에 따라 Q-테이블의 모양이 달라지게 된다.\n",
    "한 state의 여러 action 중에서 1 값을 갖는 건 무조건 하나일 수 밖에 없다.\n",
    "일단 1이 채워지면 그 다음부터는 max값을 따라서 1이 채워진 방향으로만 지나갈 것이기 때문이다.\n",
    "또한, 아무리 빠른 지름길이 있더라도, 1이 채워지지 않게 되면 그 길로 들어서지 못하게 되기 때문에 성공하더라도 최단거리일 가능성은 낮다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np      ####  http://yujuwon.tistory.com/entry/NumPy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순서\n",
    "### 1. Q table 만들기\n",
    "### 2. 게임을 진행하며 Q 테이블 업데이트 하기\n",
    "### 3. Q테이블로 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### page 163"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Q table 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.zeros((16, 4))\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))    # (16,4) : 4*4 map + 상하좌우 4개\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### page.164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 학습시키며 Q table 업데이트 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 초기화\n",
    "num_episodes = 1000\n",
    "learning_rate = 0.3\n",
    "discount_rate = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번 진행중..\n",
      "500 번 진행중..\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "for epi in range(num_episodes):\n",
    "    observation = env.reset() # 초기화\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    for t in range(100):\n",
    "        # 1턴 실행 후의 위치\n",
    "        current_state = observation\n",
    "        \n",
    "        # 학습을 빨리 하기 위해서 넣는 코드\n",
    "        # 없으면 계속 LEFT로 가기 때문\n",
    "        # 전체가 0이면 처리\n",
    "        if not Q[current_state, :].any():\n",
    "            action = env.action_space.sample()\n",
    "        elif np.random.rand() < 0.5:\n",
    "            # 랜덤 선택\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # 최대값\n",
    "            action = np.argmax(Q[current_state])\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # 성공 사례를 만들어야 측정이 가능함.\n",
    "        # 현 위치의 최대값 = 성공 가능성\n",
    "        # 이전 위치의 현 방향 선택 = 기존의 성공 가능성\n",
    "        Q[current_state, action] += learning_rate * (reward + discount_rate * np.max(Q[observation, :]) - Q[current_state, action]) \n",
    "        \n",
    "        if done:\n",
    "            episode_reward +=reward\n",
    "            break\n",
    "    if epi % 500 == 0:\n",
    "        print(epi, \"번 진행중..\")\n",
    "    total_reward += episode_reward\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.265"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward/ num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94148015, 0.95099005, 0.94387816, 0.94148015],\n",
       "       [0.94148015, 0.        , 0.95741776, 0.93558652],\n",
       "       [0.61566823, 0.97004881, 0.4838126 , 0.91110315],\n",
       "       [0.6930384 , 0.        , 0.21609863, 0.17976737],\n",
       "       [0.95099005, 0.96059601, 0.        , 0.94148015],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.98008768, 0.        , 0.8549699 ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.96059601, 0.        , 0.970299  , 0.95099005],\n",
       "       [0.96059601, 0.9801    , 0.9801    , 0.        ],\n",
       "       [0.97011416, 0.99      , 0.        , 0.96541822],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9800999 , 0.99      , 0.970299  ],\n",
       "       [0.98009994, 0.98999995, 1.        , 0.98009998],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습이 끝난 Q값\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Q테이블로 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p.166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 초기화\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0.0\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    for t in range(100):\n",
    "        #1턴 후의 위치\n",
    "        current_state = observation\n",
    "        \n",
    "        # Q값이 최대가 되는 행동을 선택함\n",
    "        action = np.argmax(Q[current_state])\n",
    "        \n",
    "        # 1턴 실행\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        #종료\n",
    "        if done:\n",
    "            episode_reward += reward\n",
    "            \n",
    "    # 총 reward\n",
    "    total_reward += episode_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000.0\n"
     ]
    }
   ],
   "source": [
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "total_reward / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stochastic 환경."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Cannot re-register id: FrozenLake-v2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-eb6fdce8a638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# 'is_slippery': True 이면 Stochastic 환경.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m'map_name'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'4x4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 4x4 크기의 맵\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;34m'is_slippery'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# 미끄러질 가능성 없음 (나중에 다룰 것)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     }\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;32m~/Tweakers/tweakers-Lecture/실습자료/machine-learning/venv/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tweakers/tweakers-Lecture/실습자료/machine-learning/venv/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, id, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot re-register id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Cannot re-register id: FrozenLake-v2"
     ]
    }
   ],
   "source": [
    "# Gym 환경설정\n",
    "# https://gym.openai.com/envs/ 에 들어가면 다양한 환경을 살펴볼 수 있음\n",
    "# Frozen-Lake : https://gym.openai.com/envs/FrozenLake-v0/ \n",
    "register(\n",
    "    id = 'FrozenLake-v2', \n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv', \n",
    "    kwargs={\n",
    "        # 'is_slippery': False 이면 Deteministic 환경.\n",
    "        # 'is_slippery': True 이면 Stochastic 환경.\n",
    "        'map_name' : '4x4', # 4x4 크기의 맵\n",
    "        'is_slippery' : True # 미끄러질 가능성 없음 (나중에 다룰 것)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v2\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위치:  4\n",
      "보상:  0.0\n",
      "Game Over?:  False\n",
      "정보(확률):  {'prob': 0.3333333333333333}\n",
      "행동:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"위치: \", observation)\n",
    "print(\"보상: \",reward)\n",
    "print(\"Game Over?: \",done)\n",
    "print(\"정보(확률): \", info) # 내가 의도한대로 갈 확률\n",
    "print(\"행동: \", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순서\n",
    "### 1. Q table 만들기\n",
    "### 2. 게임을 진행하며 Q 테이블 업데이트 하기\n",
    "### 3. Q테이블로 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### page 163"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Q table 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# np.zeros((16, 4))\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))    # (16,4) : 4*4 map + 상하좌우 4개\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### page.164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 학습시키며 Q table 업데이트 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 초기화\n",
    "num_episodes = 300000\n",
    "learning_rate = 0.3\n",
    "discount_rate = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번 진행중..\n",
      "10000 번 진행중..\n",
      "20000 번 진행중..\n",
      "30000 번 진행중..\n",
      "40000 번 진행중..\n",
      "50000 번 진행중..\n",
      "60000 번 진행중..\n",
      "70000 번 진행중..\n",
      "80000 번 진행중..\n",
      "90000 번 진행중..\n",
      "100000 번 진행중..\n",
      "110000 번 진행중..\n",
      "120000 번 진행중..\n",
      "130000 번 진행중..\n",
      "140000 번 진행중..\n",
      "150000 번 진행중..\n",
      "160000 번 진행중..\n",
      "170000 번 진행중..\n",
      "180000 번 진행중..\n",
      "190000 번 진행중..\n",
      "200000 번 진행중..\n",
      "210000 번 진행중..\n",
      "220000 번 진행중..\n",
      "230000 번 진행중..\n",
      "240000 번 진행중..\n",
      "250000 번 진행중..\n",
      "260000 번 진행중..\n",
      "270000 번 진행중..\n",
      "280000 번 진행중..\n",
      "290000 번 진행중..\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "recent_reward = []\n",
    "recent_size = 100\n",
    "\n",
    "for epi in range(num_episodes):\n",
    "    observation = env.reset() # 초기화\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    for t in range(100):\n",
    "        # 1턴 실행 후의 위치\n",
    "        current_state = observation\n",
    "        \n",
    "        # 학습을 빨리 하기 위해서 넣는 코드\n",
    "        # 없으면 계속 LEFT로 가기 때문\n",
    "        # 전체가 0이면 처리\n",
    "        \n",
    "        # if not Q[current_state, :].any():\n",
    "        # action = env.action_space.sample()\n",
    "        if np.random.rand() < 0.1: # 변경 된걸 확인하세요.\n",
    "            # 랜덤 선택\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # 최대값\n",
    "            action = np.argmax(Q[current_state])\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # 성공 사례를 만들어야 측정이 가능함.\n",
    "        # 현 위치의 최대값 = 성공 가능성\n",
    "        # 이전 위치의 현 방향 선택 = 기존의 성공 가능성\n",
    "        Q[current_state, action] += learning_rate * (reward + discount_rate * np.max(Q[observation, :]) - Q[current_state, action]) \n",
    "        \n",
    "        if done:\n",
    "            episode_reward +=reward\n",
    "            break\n",
    "    if epi % 10000 == 0:\n",
    "        print(epi, \"번 진행중..\")\n",
    "    total_reward += episode_reward\n",
    "    \n",
    "    recent_reward.append(episode_reward)\n",
    "    if len(recent_reward) >= recent_size:\n",
    "        # 최근 100개에 대해서 검사\n",
    "        recent_reward.pop(0)\n",
    "        \n",
    "        if sum(recent_reward) / recent_size > 0.7:\n",
    "            print(\"0.7이 넘었습니다. 멈추겠습니다.\")\n",
    "            break\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3347"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward/ num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recent_reward)/recent_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57518427, 0.46972455, 0.51374281, 0.47221161],\n",
       "       [0.0664377 , 0.23360794, 0.35288834, 0.46511191],\n",
       "       [0.27605549, 0.34324715, 0.32093682, 0.40912368],\n",
       "       [0.31749354, 0.15824233, 0.23682097, 0.37376393],\n",
       "       [0.60826663, 0.36966638, 0.18275754, 0.45788789],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.1146231 , 0.13135162, 0.30698845, 0.00415065],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.55898479, 0.38577184, 0.13200961, 0.65689771],\n",
       "       [0.56632572, 0.69057155, 0.45768803, 0.46736318],\n",
       "       [0.39329225, 0.45478775, 0.17288841, 0.17863629],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.51893475, 0.5204799 , 0.86332762, 0.58054121],\n",
       "       [0.72881053, 0.97853049, 0.71667287, 0.65124712],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습이 끝난 Q값\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Q테이블로 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p.166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 횟수 초기화\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0.0\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    for t in range(100):\n",
    "        #1턴 후의 위치\n",
    "        current_state = observation\n",
    "        \n",
    "        # Q값이 최대가 되는 행동을 선택함\n",
    "        action = np.argmax(Q[current_state])\n",
    "        \n",
    "        # 1턴 실행\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        #종료\n",
    "        if done:\n",
    "            episode_reward += reward\n",
    "            \n",
    "    # 총 reward\n",
    "    total_reward += episode_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591.0\n"
     ]
    }
   ],
   "source": [
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.591"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN ( p.167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Stochastic 환경.\n",
    "env = gym.make('FrozenLake-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 구성 p.169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(16, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 96)\n",
    "        self.fc4 = nn.Linear(96, 96)\n",
    "        self.fc5 = nn.Linear(96, 64)\n",
    "        self.fc6 = nn.Linear(64, 64)\n",
    "        self.fc7 = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onehot2tensor, applymodel p. 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2tensor(state):\n",
    "    tmp = np.zeros(16)\n",
    "    tmp[state] = 1\n",
    "    vector = np.array(tmp, dtype='float32')\n",
    "    tensor = torch.from_numpy(vector).float()\n",
    "    return tensor\n",
    "\n",
    "def applymodel(tensor):\n",
    "    output_tensor = model(tensor)\n",
    "    output_array = output_tensor.data.numpy()\n",
    "    return output_tensor, output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "for x in range(16):\n",
    "    print(onehot2tensor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화(학습) - (p.171)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss가 수렴하지 않음 (책)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0498, 0.0501, 0.0498, 0.0503], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0498, 0.0501, 0.0498, 0.0502], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0497, 0.0501, 0.0498, 0.0501], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0497, 0.0500, 0.0497, 0.0501], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0496, 0.0500, 0.0497, 0.0500], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0496, 0.0500, 0.0497, 0.0499], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0496, 0.0499, 0.0497, 0.0499], grad_fn=<AddBackward0>)\n",
      "500번: total_loss: 4.390981374058356e-07, total_reward: 4.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0828, 0.0827, 0.0834, 0.0834], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0827, 0.0827, 0.0833, 0.0834], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0826, 0.0827, 0.0833, 0.0833], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0825, 0.0827, 0.0833, 0.0833], grad_fn=<AddBackward0>)\n",
      "1000번: total_loss: 6.945425639059977e-07, total_reward: 11.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2013, 0.1906, 0.2013, 0.1896], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2013, 0.1914, 0.2013, 0.1898], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2013, 0.1921, 0.2013, 0.1901], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2013, 0.1928, 0.2013, 0.1903], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2012, 0.1934, 0.2012, 0.1905], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2011, 0.1939, 0.2012, 0.1907], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2009, 0.1944, 0.2012, 0.1908], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2008, 0.1948, 0.2012, 0.1910], grad_fn=<AddBackward0>)\n",
      "1500번: total_loss: 8.100931950139056e-06, total_reward: 23.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0537, 0.0533, 0.0544, 0.0545], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0536, 0.0532, 0.0544, 0.0544], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0536, 0.0531, 0.0543, 0.0544], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0536, 0.0531, 0.0543, 0.0543], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0536, 0.0530, 0.0543, 0.0543], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0536, 0.0530, 0.0542, 0.0542], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0536, 0.0530, 0.0542, 0.0542], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.0536, 0.0529, 0.0542, 0.0541], grad_fn=<AddBackward0>)\n",
      "2000번: total_loss: 5.172637994044216e-07, total_reward: 26.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2061, 0.2237, 0.1579, 0.1871], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2071, 0.2243, 0.1580, 0.1872], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2080, 0.2249, 0.1581, 0.1872], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2088, 0.2254, 0.1582, 0.1873], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2096, 0.2258, 0.1583, 0.1873], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2102, 0.2262, 0.1584, 0.1873], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "tensor([0.2108, 0.2266, 0.1585, 0.1873], grad_fn=<AddBackward0>)\n",
      "2500번: total_loss: 8.880250106813037e-06, total_reward: 41.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.3956, 0.4090, 0.3852, 0.3968], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.3956, 0.4089, 0.3852, 0.3968], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.3956, 0.4089, 0.3853, 0.3968], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.3956, 0.4089, 0.3853, 0.3968], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.3956, 0.4088, 0.3853, 0.3969], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.3956, 0.4088, 0.3853, 0.3969], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.3956, 0.4087, 0.3853, 0.3969], grad_fn=<AddBackward0>)\n",
      "3000번: total_loss: 2.925416538346326e-05, total_reward: 66.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4462, 0.4473, 0.4472, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4462, 0.4472, 0.4472, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4458, 0.4472, 0.4472, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4454, 0.4471, 0.4470, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4451, 0.4471, 0.4468, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4448, 0.4470, 0.4467, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4445, 0.4469, 0.4465, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4443, 0.4469, 0.4464, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4440, 0.4468, 0.4462, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4436, 0.4468, 0.4461, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4433, 0.4467, 0.4460, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4429, 0.4467, 0.4459, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4426, 0.4466, 0.4459, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4424, 0.4465, 0.4458, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4421, 0.4465, 0.4457, 0.4445], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4419, 0.4464, 0.4457, 0.4444], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4417, 0.4464, 0.4456, 0.4444], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4416, 0.4463, 0.4456, 0.4444], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4414, 0.4463, 0.4455, 0.4444], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.4413, 0.4462, 0.4455, 0.4443], grad_fn=<AddBackward0>)\n",
      "3500번: total_loss: 8.639251262820835e-05, total_reward: 82.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5517, 0.5828, 0.5697, 0.5387], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5517, 0.5832, 0.5703, 0.5387], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5517, 0.5836, 0.5708, 0.5387], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5517, 0.5840, 0.5713, 0.5387], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5517, 0.5843, 0.5717, 0.5388], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5517, 0.5846, 0.5720, 0.5388], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.5517, 0.5848, 0.5724, 0.5388], grad_fn=<AddBackward0>)\n",
      "4000번: total_loss: 6.19894726696657e-05, total_reward: 109.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6703, 0.6756, 0.6720, 0.6250], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6702, 0.6755, 0.6720, 0.6250], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6702, 0.6755, 0.6720, 0.6250], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6702, 0.6754, 0.6720, 0.6250], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6700, 0.6753, 0.6720, 0.6250], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6698, 0.6753, 0.6720, 0.6250], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "tensor([0.6697, 0.6752, 0.6720, 0.6250], grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "tensor([0.6695, 0.6752, 0.6717, 0.6250], grad_fn=<AddBackward0>)\n",
      "4500번: total_loss: 7.212693770952683e-05, total_reward: 134.0 \n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6642, 0.6845, 0.6577, 0.6655], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6642, 0.6850, 0.6577, 0.6655], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6642, 0.6855, 0.6577, 0.6655], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6642, 0.6859, 0.6577, 0.6655], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6642, 0.6863, 0.6577, 0.6655], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6642, 0.6866, 0.6577, 0.6655], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6642, 0.6869, 0.6577, 0.6655], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6642, 0.6872, 0.6577, 0.6655], grad_fn=<AddBackward0>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6642, 0.6874, 0.6577, 0.6655], grad_fn=<AddBackward0>)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0.6642, 0.6876, 0.6577, 0.6655], grad_fn=<AddBackward0>)\n",
      "5000번: total_loss: 0.00011775045368267456, total_reward: 153.0 \n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "num_episodes = 5000\n",
    "discount_rate = 0.99\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    # 현재 게임 reward\n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    # 누적 오차\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for t in range(100):\n",
    "        current_state = observation\n",
    "        \n",
    "        # 경사 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 현위치 onehot encoding \n",
    "        current_tensor = onehot2tensor(current_state)\n",
    "\n",
    "        if (i_episode+1) % 500 == 0:\n",
    "            print(current_tensor)\n",
    "            current_output_tensor, current_output_array = applymodel(current_tensor)\n",
    "            print(current_output_tensor)\n",
    "            # print(\"{}번: total_loss: {}, total_reward: {} \".format(i_episode+1, total_loss, total_reward))\n",
    "            \n",
    "            \n",
    "        # 모형에 입력\n",
    "        current_output_tensor, current_output_array = applymodel(current_tensor)\n",
    "        # 행동 선택\n",
    "        if np.random.rand() < 0.1:\n",
    "            #무작위 선택\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Q값이 최대가 되도록\n",
    "            action = np.argmax(current_output_array)\n",
    "\n",
    "        ## 1턴 실행\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # onehot 벡터 텐서로 변환\n",
    "        observation_tensor = onehot2tensor(observation)\n",
    "        \n",
    "        # 모형에 입력\n",
    "        observation_output_tensor, observation_output_array = applymodel(observation_tensor)\n",
    "        \n",
    "        # Q값 업데이트\n",
    "        # y값\n",
    "        q = reward + discount_rate * np.max(observation_output_array) # reward는 마지막 value를 위해 더함\n",
    "        \n",
    "        # 기존 보드의 q값 복사\n",
    "        q_array = np.copy(current_output_array) # 이전 값\n",
    "        q_array[action] = q\n",
    "        q_variable = torch.from_numpy(q_array)\n",
    "        \n",
    "        # 오차 계산 - q_array와 q\n",
    "        # 현재 아웃풋 값과, 기존 q어레이\n",
    "        # y값은 올바른 선택일 때의 최대값,\n",
    "        # q^은 현재 선택의 결과\n",
    "        \n",
    "        # current_output_tensor: 현재 측정값\n",
    "        # q_variable: 최고의 선택을 했을 때 y값\n",
    "        loss = criterion(current_output_tensor, q_variable)\n",
    "        \n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        \n",
    "        # 웨이트 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 오차 누적 계산\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if done:\n",
    "            # 종료\n",
    "            episode_reward += reward\n",
    "            \n",
    "            break\n",
    "    total_reward += episode_reward\n",
    "    \n",
    "    # 누적오차 및 보상률\n",
    "    if (i_episode+1) % 500 == 0:\n",
    "        print(\"{}번: total_loss: {}, total_reward: {} \".format(i_episode+1, total_loss, total_reward))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQL 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500번:\n",
      "24.0\n",
      "------------------------------\n",
      "1000번:\n",
      "53.0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0.0\n",
    "num_episodes = 1000\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    # 현재 게임 reward\n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    \n",
    "    for t in range(100):\n",
    "        current_state = observation\n",
    "        \n",
    "        \n",
    "        # 현위치 onehot encoding \n",
    "        current_tensor = onehot2tensor(current_state)\n",
    "        # 모형에 입력\n",
    "        current_output_tensor, current_output_array = applymodel(current_tensor)\n",
    "        \n",
    "        # Q테이블에서 제일 큰거 선택\n",
    "        action = np.argmax(current_output_array)\n",
    "\n",
    "        ## 1턴 실행\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # 종료\n",
    "            episode_reward += reward\n",
    "            break\n",
    "            \n",
    "    total_reward += episode_reward\n",
    "    \n",
    "    # 누적오차 및 보상률\n",
    "    if (i_episode+1) % 500 == 0:\n",
    "        print(\"{}번:\".format(i_episode+1))\n",
    "        print(total_reward)\n",
    "        print(\"-\"*30)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.0\n",
      "0.053\n"
     ]
    }
   ],
   "source": [
    "print(total_reward)  \n",
    "print(total_reward/1000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
