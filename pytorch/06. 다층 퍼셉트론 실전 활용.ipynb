{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from torch.autograd import Variable # Deprecated -> Torch에 지원\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset # DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Wine 데이터 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "wine.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, wine.target_names, wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = wine.data[:130]\n",
    "wine_target = wine.target[:130]\n",
    "wine_data, wine_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X, test_X, train_y, test_y = train_test_split(wine_data, wine_target, test_size=.2)\n",
    "train_X, test_X, train_y, test_y = train_test_split(wine_data, wine_target, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Code\n",
    "# tensor from numpy\n",
    "train_X = torch.from_numpy(train_X).float() # double(float64)->float(float32)\n",
    "train_y = torch.from_numpy(train_y).long() # long -> long (int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = torch.from_numpy(test_X).float()\n",
    "test_y = torch.from_numpy(test_y).long() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.size()) # ->104행 13열 (2차원배열)\n",
    "print(train_y.size()) # ->104행 1열  (1차원배열)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Code\n",
    "train = TensorDataset(train_X, train_y) # like Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_X\", train.tensors[0].size())\n",
    "print(\"train_y\", train.tensors[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Code\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True) \n",
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 구성 - P.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 96)\n",
    "        self.fc2 = nn.Linear(96, 96)\n",
    "        self.fc3 = nn.Linear(96, 96)\n",
    "        self.fc4 = nn.Linear(96, 96)\n",
    "        self.fc5 = nn.Linear(96, 96)\n",
    "        self.fc6 = nn.Linear(96, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model Instance 생성\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모형 학습 - P.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss(Cost) Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# metric = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Decent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) # learning late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for train_x, train_y in train_loader:\n",
    "        # train_x, train_y = Variable(train_x), Variable(train_y) # Deprecated\n",
    "        # train_x.requires_grad_()\n",
    "        \n",
    "        # RuntimeError: only Tensors of floating point dtype can require gradients\n",
    "        # train_y = train_y.float()\n",
    "        # train_y.requires_grad_()\n",
    "        # train_y = Variable(train_y)\n",
    "        \n",
    "        optimizer.zero_grad() # reset gradient\n",
    "        \n",
    "        output = model(train_x)\n",
    "        \n",
    "        loss = criterion(output, train_y)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # total_loss += loss.data[0]\n",
    "        total_loss += loss.item()\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(epoch+1, total_loss)\n",
    "        print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 출력이 0 혹은 1이 되게 함\n",
    "result = torch.max(model(test_X), 1)[1]\n",
    "# result = torch.max(model(test_X), 1).indices\n",
    "\n",
    "#정확도 측정\n",
    "accuracy = sum(test_y.data.numpy() == result.numpy()) / len(test_y.data.numpy())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MNIST 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# from torch.autograd import Variable  # Deprecated\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib 라이브러리 임포트\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mldata.org is no longer operational.\n",
    "# mnist = datasets.fetch_mldata('MNIST original', data_home='./data/') # deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.openml.org\n",
    "# https://www.openml.org/d/554\n",
    "mnist = datasets.fetch_openml(name='mnist_784', data_home='./data/') # 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.data.min(), mnist.data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 값의 범위가 0~255\n",
    "# 정규화 0~1사이로 만듬\n",
    "mnist_data = mnist.data / 255\n",
    "\n",
    "# column 784 --> 28*28\n",
    "mnist_df = pd.DataFrame(mnist_data)\n",
    "mnist_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.85:\n",
    "# 데이터가 없음.\n",
    "plt.imshow(mnist_data[0].reshape(28, 28), cmap=cm.gray_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_label = mnist.target\n",
    "mnist_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7만개\n",
    "mnist_label.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 5000\n",
    "\n",
    "test_size = 500\n",
    "\n",
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(\n",
    "    mnist_data, mnist_label, train_size=train_size, test_size=test_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서 생성 - p.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = torch.from_numpy(train_X).float()\n",
    "# train_y = torch.from_numpy(train_y).long() Error\n",
    "train_y = torch.from_numpy(train_y.astype(np.int)).long()\n",
    "\n",
    "test_X = torch.from_numpy(test_X).float()\n",
    "test_y = torch.from_numpy(test_y.astype(np.int)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature + label\n",
    "# DataLoader를 사용하기 위해 하는 작업\n",
    "train = TensorDataset(train_X, train_y)\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "# DataLoader: 미니배치를 가능하게 해줌.\n",
    "train_loader = DataLoader(train, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 128)\n",
    "        self.fc6 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc6(x)\n",
    "        return F.log_softmax(x, dim=0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모형 학습 - p.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 함수\n",
    "# Cross Entropy 손실함수(cost function)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저\n",
    "# Stochastic Gradient Descent\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) # lr = learning rate\n",
    "\n",
    "# 학습 시작\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for train_x, train_y in train_loader:\n",
    "        # grad 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward - 순전파\n",
    "        output = model(train_x)\n",
    "        \n",
    "        loss = criterion(output, train_y)\n",
    "        # backward - 역전파\n",
    "        loss.backward()\n",
    "        \n",
    "        # 가중치 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 오차 계산\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    if (epoch +1) % 100 == 0:\n",
    "        print(epoch+1, total_loss)\n",
    "        \n",
    "# 오래 걸려요.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classfication (예측)\n",
    "# torch.exp(model(test_X).data)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mnist_data[0].reshape(28, 28), cmap=cm.gray_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(model(test_X).data, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.max(model(test_X).data, 1)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도\n",
    "accuracy = sum(test_y.data.numpy() == result.numpy())/ len(test_y.data.numpy())\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 기사 분류 - p.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습데이터 준비 p.20\n",
    "- http://kristalinfo.com/TestCollections/#hkib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://kristalinfo.com/TestCollections/#hkib\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "gzip_url = 'http://kristalinfo.com/download/hkib-20000-40075.tar.gz'\n",
    "\n",
    "filename = './data/hkib/hkib-20000-40075.tar.gz'\n",
    "base_dir = os.path.dirname(filename)\n",
    "\n",
    "if not os.path.exists(os.path.dirname(filename)):\n",
    "    os.makedirs(os.path.dirname(filename))\n",
    "\n",
    "# 크롤링\n",
    "with requests.get(gzip_url, stream=True) as resp:\n",
    "    with open(filename, 'wb') as f:\n",
    "        while True:\n",
    "            chunk = resp.raw.read(1024, decode_content=True)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f.write(chunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압축 풀기\n",
    "with tarfile.open(filename, \"r:gz\") as tar:\n",
    "    tar.extractall(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 import - p.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# 정규표현식\n",
    "import re\n",
    "\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\student\\desktop\\multicampus_lecture\\pytorch\\jpype1-0.6.2-cp36-cp36m-win_amd64.whl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Requirement 'JPype1-0.6.2-cp36-cp36m-win_amd64.whl' looks like a filename, but the file does not exist\n",
      "ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\student\\\\Desktop\\\\multicampus_lecture\\\\pytorch\\\\JPype1-0.6.2-cp36-cp36m-win_amd64.whl'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install JPype1-0.6.2-cp36-cp36m-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install JPype1\n",
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konlpy\n",
    "# 형태소 분석기 \n",
    "try:\n",
    "    from konlpy.tag import Hannanum, Kkma\n",
    "except ModuleNotFoundError as e:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install konlpy\n",
    "finally:\n",
    "    from konlpy.tag import Hannanum, Kkma\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "target_dir = os.path.join(base_dir, 'HKIB-20000')\n",
    "cat_dirs = ['health', 'economy', 'science', 'education', 'culture', 'society', 'industry', 'leisure', 'politics']\n",
    "cat_prefixes = ['건강', '경제', '과학', '교육', '문화', '사회', '산업', '여가', '정치']\n",
    "\n",
    "files = os.listdir(target_dir)\n",
    "print(files)\n",
    "\n",
    "for file in files:\n",
    "    if not file.endswith('.txt'):\n",
    "        continue\n",
    "\n",
    "    # 기사 단위 분할 -> 리스트\n",
    "    with open(target_dir + '/' + file) as curfile:\n",
    "        doc_cnt = 0\n",
    "        docs = []\n",
    "        curr_doc = None\n",
    "        \n",
    "        for curr_line in curfile:\n",
    "            # 매 라인 읽기\n",
    "            if curr_line.startswith('@DOCUMENT'):\n",
    "                # 시작점 찾기\n",
    "                if curr_doc is not None:\n",
    "                    # 읽고 있는 문서가 있으면 붙이기\n",
    "                    docs.append(curr_doc)\n",
    "                    \n",
    "                curr_doc = curr_line\n",
    "                doc_cnt = doc_cnt +1\n",
    "                continue\n",
    "            curr_doc = curr_doc + curr_line\n",
    "\n",
    "        # 마지막 것 넣어줘야함 (책 오류)\n",
    "        docs.append(curr_doc)\n",
    "        print(len(docs), docs[-1]==docs[-2])\n",
    "\n",
    "    # 기사 분류\n",
    "    for doc in docs:\n",
    "        doc_lines = doc.split('\\n')\n",
    "        doc_no = doc_lines[1][9:] # doc_lines[1].split(':')[1].strip()\n",
    "        \n",
    "        # 주제 추출\n",
    "        doc_cat03 = ''\n",
    "        for line in doc_lines[:10]:\n",
    "            if line.startswith(\"#CAT'03\"):\n",
    "                doc_cat03 = line[10: ]\n",
    "                break\n",
    "        \n",
    "        # 주제별로 디렉토리 정리\n",
    "        for cat_prefix in cat_prefixes:\n",
    "            if doc_cat03.startswith(cat_prefix):\n",
    "                # prefix로 시작하는지 확인\n",
    "                dir_index = cat_prefixes.index(cat_prefix) # index 맞춰서 저장 --> 폴더명은 영문\n",
    "                break\n",
    "        \n",
    "        # 문서 정보 제거, 기사 본문만 남기기\n",
    "        filtered_lines = []\n",
    "        for line in doc_lines:\n",
    "            if not (line.startswith('#') or line.startswith('@')):\n",
    "                # 메타태그 제외 append\n",
    "                filtered_lines.append(line)\n",
    "        \n",
    "        filepath = os.path.join(target_dir, cat_dirs[dir_index])\n",
    "        filename = os.path.join(filepath, 'hkib-' + doc_no + '.txt')\n",
    "        \n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write('\\n'.join(filtered_lines))\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 6.16~6.17\n",
    "# 예제 6.17로 진행. (6.16은 의미X)\n",
    "dirs = ['education', 'health']\n",
    "\n",
    "# 기사에 출현하는 단어와 레이블 저장 리스트\n",
    "# features\n",
    "x_ls = []\n",
    "\n",
    "# target_values\n",
    "y_ls = []\n",
    "\n",
    "# 필요없음\n",
    "# tmp1 = [] # 임시 토큰 저장소\n",
    "# tmp2 = '' # 임시 문자열 저장소\n",
    "tmp = ''\n",
    "\n",
    "# tokenizer = Hannnum()\n",
    "tokenizer = Kkma()\n",
    "\n",
    "#각 폴더의 파일을 하나씩 읽음\n",
    "for i, d in enumerate(dirs):\n",
    "    \n",
    "    # 해당 디렉토리에 파일들 ['education', 'health']\n",
    "    base_dir = target_dir + '/' + d\n",
    "    files = os.listdir(base_dir)\n",
    "    \n",
    "    for fid, file in enumerate(files):\n",
    "        with open(base_dir + '/' + file, 'r', encoding='utf-8') as f:\n",
    "            raw = f.read()\n",
    "            \n",
    "            # reg_raw = re.sub(r'[0-9a-zA-Z]', '', raw) # 숫자 및 영어 제거 (? - 안해도 괜찮아 보임)\n",
    "            reg_raw = re.sub(r'[-\\'@#:/◆▲0-9a-zA-Z<>!-\"*\\(\\)]', '', raw) # 윈도우는 [ㅁ+한자키], 맥은 [cmd+ctrl+space\n",
    "            \n",
    "            reg_raw = re.sub(r'[ ]+', ' ', reg_raw)\n",
    "            reg_raw = reg_raw.replace('\\n', ' ') # 줄바꿈 없음\n",
    "        \n",
    "        # 형태소 분석 -> 명사만 추출하여 리스트 생성\n",
    "        tokens = tokenizer.nouns(reg_raw)\n",
    "        \n",
    "        tmp = ' '.join(tokens)\n",
    "        \n",
    "        # 필요 없음\n",
    "        # for token in tokens: # 토큰 저장\n",
    "        # tmp1.append(token)\n",
    "        \n",
    "        x_ls.append(tmp)\n",
    "        y_ls.append(i) # 기사 주제 레이블 (label index)\n",
    "        if fid % 50 == 0:\n",
    "            print(\"{}-[{}/{}] 진행중...\".format(i+1, fid+1, len(files)+1))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_ls), len(y_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_ls).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_ls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## numpy 배열\n",
    "x_array = np.array(x_ls)\n",
    "y_array = np.array(y_ls)\n",
    "\n",
    "## 단어 출현 횟수 세기\n",
    "# sklearn 변환기 # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "cntvec = CountVectorizer() \n",
    "x_cntvecs = cntvec.fit_transform(x_array)\n",
    "x_cntarray = x_cntvecs.toarray() # matrix.toarray()\n",
    "\n",
    "# 데이터 프레임으로 변환\n",
    "df = pd.DataFrame(x_cntarray)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 단어와 단어의 인덱스 표시\n",
    "\n",
    "## CountVectorizer().vocabulary_: 단어와 인덱스 매핑\n",
    "## dict 형식: { '<단어>': <index>, ...}\n",
    "for k, v in sorted(cntvec.vocabulary_.items(), key=lambda x: x[1]):\n",
    "    # 사전식 정렬\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer =>\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "tfidf_vec = TfidfVectorizer(use_idf=True)\n",
    "x_tfidf_vecs = tfidf_vec.fit_transform(x_array)\n",
    "x_tfidf_array = x_tfidf_vecs.toarray()\n",
    "\n",
    "pd.DataFrame(x_tfidf_array).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(x_tfidf_array, y_array, test_size=.2, random_state=42)\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서 생성 - p.106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 임포트\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.from_numpy(train_X,).float()\n",
    "# train_y = torch.from_numpy(train_y).long()\n",
    "train_y = torch.from_numpy(train_y).long() # only \n",
    "\n",
    "test_X = torch.from_numpy(test_X,).float()\n",
    "test_y = torch.from_numpy(test_y).long() # only \n",
    "\n",
    "train_X.size(), train_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TensorDataset(train_X, train_y)\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(33572, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 128)\n",
    "        self.fc6 = nn.Linear(128, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc5(x)\n",
    "        return F.log_softmax(x, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "# model.cuda() 우린 GPU가 없어요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습하기 - p.111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer: (Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# learning\n",
    "for epoch in range(100):\n",
    "    total_loss =0\n",
    "    \n",
    "    # 미니배치\n",
    "    for train_x, train_y in train_loader:\n",
    "        # train_x.requires_grad_()\n",
    "        # train_y.requires_grad_()\n",
    "\n",
    "        # slope init\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        output = model(train_x)\n",
    "        \n",
    "        # 오차\n",
    "        loss = criterion(output, train_y)\n",
    "        \n",
    "        # backward(역전파)  계산\n",
    "        loss.backward()\n",
    "        # 가중치 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(epoch+1, total_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = Variable(test_X), Variable(test_y)\n",
    "result = torch.max(model(test_x).data, 1)[1] # 모델에서 가장 큰 값으로\n",
    "\n",
    "# test_y.cpu() 우린 GPU를 안 썼다.\n",
    "# accuracy = sum(test_y.data.cpu().numpy() == result.cpu().numpy()) / len(test_y.cpu().data.numpy())\n",
    "accuracy = sum(test_y.data.numpy() == result.numpy()) / len(test_y.data.numpy())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시계열 데이터에서 이상탐지하기 - p.114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2011. 1월 1일부터 2019년 06월 25일까지 데이터\n",
    "#### https://data.kma.go.kr/data/grnd/selectAsosRltmList.do?pgmNo=36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# 정규표현식\n",
    "import re\n",
    "\n",
    "from sklearn import datasets, model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = './data/weather'\n",
    "filename = '20110101_20190515.csv'\n",
    "filepath = os.path.join(target_dir, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from torch.autograd import Variable # Deprecated\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(filepath, encoding='cp949')\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5년\n",
    "temp = dat['평균기온(°C)']\n",
    "\n",
    "temp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5년\n",
    "train_x = temp[:int(365*6.5)]\n",
    "test_x = temp[int(365*6.5):]\n",
    "\n",
    "train_x = np.array(train_x)\n",
    "test_x = np.array(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x), len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTR_SIZE = 180 \n",
    "train_x[0:0+ATTR_SIZE][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[1:1+ATTR_SIZE][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTR_SIZE = 180 # 윈도우 폭\n",
    "\n",
    "tmp=[]\n",
    "train_X = []\n",
    "\n",
    "for i in range(0, len(train_x) - ATTR_SIZE):\n",
    "    tmp.append(train_x[i:i+ATTR_SIZE]) ## 윈도우 폭만큼 데이터 저장.\n",
    "\n",
    "train_X = np.array(tmp)\n",
    "df = pd.DataFrame(train_X)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(180, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 128)\n",
    "        self.fc4 = nn.Linear(128, 180)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모형 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optim.SGD\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(7000):\n",
    "    total_loss = 0\n",
    "    d = []\n",
    "    \n",
    "    for i in range(100):\n",
    "        # 훈련 데이터에 랜덤으로 인덱스 부여\n",
    "        index = np.random.randint(0, len(train_X))\n",
    "        \n",
    "        #미니배치 분할\n",
    "        d.append(train_X[index])\n",
    "    d = np.array(d, dtype='float32')\n",
    "    \n",
    "    d = torch.from_numpy(d)\n",
    "    \n",
    "    # zero grad\n",
    "    optimizer.zero_grad()\n",
    "    # forward\n",
    "    output=model(d)\n",
    "    \n",
    "    # loss\n",
    "    loss = criterion(output, d)\n",
    "    #backward\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(epoch+1, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(d.data[0].numpy(), label='original')\n",
    "plt.plot(output.data[0].numpy(), label='output')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이상 점수 계산 - p.126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "test_X = []\n",
    "last_i = 0\n",
    "\n",
    "# 테스트 데이터 분할 180\n",
    "print(len(test_x))\n",
    "for i in range(0, len(test_x), 180):\n",
    "    if len(test_x) - i < 180:\n",
    "        # 개수 안맞으면 break\n",
    "        break\n",
    "    tmp.append(test_x[i:i+180])\n",
    "    last_i = i+180\n",
    "\n",
    "test_X = np.array(tmp, dtype='float32')\n",
    "pd.DataFrame(test_X)\n",
    "# tmp.append(test_x[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.from_numpy(test_X)\n",
    "output = model(d)\n",
    "\n",
    "# ploting\n",
    "plt.plot(test_X.flatten(), label='original')\n",
    "plt.plot(output.data.numpy().flatten(), label='prediction')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상 점수 계산\n",
    "test = test_X.flatten()\n",
    "pred = output.data.numpy().flatten()\n",
    "\n",
    "total_score = []\n",
    "\n",
    "for i in range(0, last_i):\n",
    "    dist = test[i] - pred[i]\n",
    "    # x to the power y,\n",
    "    score = pow(dist, 2) # dist **2 \n",
    "    total_score.append(score)\n",
    "    \n",
    "# 이상 점수 정규화 [0, 1]\n",
    "total_score = np.array(total_score)\n",
    "max_score = np.max(total_score)\n",
    "total_score = total_score / max_score\n",
    "\n",
    "total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임계값을 정해둬야 함.\n",
    "plt.plot(total_score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
